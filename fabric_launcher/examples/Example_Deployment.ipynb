{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98dcf29",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Package Installation\n",
    "\n",
    "Install the fabric-launcher package and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fe0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fabric-launcher --quiet\n",
    "%pip install --upgrade azure-core azure-identity --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è Restarting Python kernel for installed packages to take effect\")\n",
    "notebookutils.session.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58353504",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Configuration\n",
    "\n",
    "Update these values to customize the deployment for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a16569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Repository Configuration\n",
    "REPO_OWNER = \"myorg\"\n",
    "REPO_NAME = \"my-fabric-solution\"\n",
    "BRANCH = \"main\"\n",
    "FOLDER_TO_EXTRACT = \"workspace\"\n",
    "\n",
    "# GitHub Personal Access Token (optional, for private repositories)\n",
    "GITHUB_TOKEN = \"\"\n",
    "\n",
    "# Deployment Configuration\n",
    "ENVIRONMENT = \"DEV\"  # Options: DEV, TEST, PROD\n",
    "DEBUG = False\n",
    "\n",
    "# Fabric API Configuration\n",
    "API_ROOT_URL = \"https://api.fabric.microsoft.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd892c",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Initialize Launcher\n",
    "\n",
    "Import the fabric-launcher package and initialize the launcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebookutils\n",
    "\n",
    "from fabric_launcher import FabricLauncher\n",
    "\n",
    "# Initialize the launcher\n",
    "launcher = FabricLauncher(\n",
    "    notebookutils,\n",
    "    environment=ENVIRONMENT,\n",
    "    api_root_url=API_ROOT_URL,\n",
    "    debug=DEBUG,\n",
    "    allow_non_empty_workspace=False,  # Safety: only deploy to empty workspaces\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fabric Launcher initialized\")\n",
    "print(f\"üìç Workspace ID: {launcher.workspace_id}\")\n",
    "print(f\"üè∑Ô∏è Environment: {launcher.environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f2e0f",
   "metadata": {},
   "source": [
    "## üì• Step 4: Download and Deploy\n",
    "\n",
    "Download the solution from GitHub and deploy all artifacts to the current workspace.\n",
    "\n",
    "This operation will:\n",
    "- Download the repository from GitHub\n",
    "- Extract the specified folder\n",
    "- Deploy Fabric items in stages (data stores first, then compute & analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Starting Download and Deployment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "launcher.download_and_deploy(\n",
    "    repo_owner=REPO_OWNER,\n",
    "    repo_name=REPO_NAME,\n",
    "    workspace_folder=FOLDER_TO_EXTRACT,\n",
    "    branch=BRANCH,\n",
    "    github_token=GITHUB_TOKEN if GITHUB_TOKEN else None,\n",
    "    staged_deployment=True,  # Deploy data stores first, then compute\n",
    "    data_folders={\"data\": \"reference-data\", \"samples\": \"sample-data\"},  # Optional\n",
    "    lakehouse_name=\"ReferenceDataLH\",  # Optional: specify if using data_folders\n",
    "    data_file_patterns=[\"*.json\", \"*.csv\", \"*.geojson\"],  # Optional\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Download and Deployment Completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f575f4",
   "metadata": {},
   "source": [
    "## üìÅ Step 5: Copy Data Folders to Lakehouse (Optional)\n",
    "\n",
    "Copy data folders from the downloaded repository to a Lakehouse.\n",
    "\n",
    "**Note:** If you used `data_folders` parameter in Step 4, this step is already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9391fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data folder copying\n",
    "LAKEHOUSE_NAME = \"ReferenceDataLH\"\n",
    "DATA_FOLDER_MAPPINGS = {\n",
    "    \"data\": \"reference-data\",  # Copy 'data' folder to 'reference-data' in Lakehouse\n",
    "    \"samples\": \"sample-data\",  # Copy 'samples' folder to 'sample-data' in Lakehouse\n",
    "}\n",
    "FILE_PATTERNS = [\"*.json\", \"*.csv\", \"*.geojson\"]  # Only copy these file types\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÅ Copying Data Folders to Lakehouse\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "\n",
    "    repository_base_path = \".lakehouse/default/Files/src\"\n",
    "\n",
    "    launcher.copy_data_folders_to_lakehouse(\n",
    "        lakehouse_name=LAKEHOUSE_NAME,\n",
    "        repository_base_path=repository_base_path,\n",
    "        folder_mappings=DATA_FOLDER_MAPPINGS,\n",
    "        file_patterns=FILE_PATTERNS,\n",
    "        recursive=True,\n",
    "    )\n",
    "    print(f\"‚úÖ Data folders copied to {LAKEHOUSE_NAME}/Files/\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Data folder copy skipped or failed: {e}\")\n",
    "    print(\"This is optional - deployment can continue without reference data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0c2c4",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Step 6: Execute Post-Deployment Notebook (Optional)\n",
    "\n",
    "Trigger a post-deployment notebook to initialize data or perform other setup tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure post-deployment notebook execution\n",
    "POST_DEPLOYMENT_NOTEBOOK = \"Initialize-Reference-Data\"\n",
    "NOTEBOOK_PARAMETERS = {\"environment\": ENVIRONMENT, \"data_folder\": TARGET_FOLDER}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ñ∂Ô∏è Executing Post-Deployment Notebook\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    result = launcher.run_notebook(notebook_name=POST_DEPLOYMENT_NOTEBOOK, parameters=NOTEBOOK_PARAMETERS)\n",
    "\n",
    "    print(\"‚úÖ Notebook execution triggered successfully\")\n",
    "    print(f\"üìì Notebook: {POST_DEPLOYMENT_NOTEBOOK}\")\n",
    "    print(f\"üÜî Job ID: {result['job_id']}\")\n",
    "    print(\"‚ÑπÔ∏è The notebook is running in the background.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Notebook execution skipped or failed: {e}\")\n",
    "    print(\"This is optional - deployment is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ce291",
   "metadata": {},
   "source": [
    "## ‚úÖ Deployment Summary\n",
    "\n",
    "Your Fabric solution has been deployed successfully!\n",
    "\n",
    "### What was deployed:\n",
    "- ‚úì Fabric workspace items (Lakehouses, Notebooks, etc.)\n",
    "- ‚úì Reference data files (if configured)\n",
    "- ‚úì Post-deployment tasks (if configured)\n",
    "\n",
    "### Next Steps:\n",
    "1. Refresh your browser to see the newly deployed items\n",
    "2. Review the deployed artifacts in your workspace\n",
    "3. Check the execution status of any triggered notebooks\n",
    "4. Verify data and configurations\n",
    "\n",
    "### Manual Steps (if any):\n",
    "- Review solution-specific documentation for any manual configuration steps\n",
    "- Update connection strings or credentials as needed\n",
    "- Configure scheduled refreshes or triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd88276",
   "metadata": {},
   "source": [
    "## üîß Advanced: Alternative Deployment Approaches\n",
    "\n",
    "The sections below show alternative ways to use fabric-launcher for more control over the deployment process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3434e1a",
   "metadata": {},
   "source": [
    "### Alternative 1: Step-by-Step Deployment\n",
    "\n",
    "This approach gives you more control over each deployment stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download from GitHub\n",
    "print(\"üì• Downloading from GitHub...\")\n",
    "launcher.download_repository(\n",
    "    repo_owner=REPO_OWNER,\n",
    "    repo_name=REPO_NAME,\n",
    "    extract_to=\".lakehouse/default/Files/src\",\n",
    "    folder_to_extract=FOLDER_TO_EXTRACT,\n",
    "    branch=BRANCH,\n",
    ")\n",
    "\n",
    "# Step 2: Deploy data stores first\n",
    "print(\"\\nüóÑÔ∏è Deploying data stores...\")\n",
    "import os\n",
    "\n",
    "repository_directory = os.path.join(\".lakehouse/default/Files/src\", FOLDER_TO_EXTRACT)\n",
    "\n",
    "from fabric_launcher import FabricDeployer\n",
    "\n",
    "deployer = FabricDeployer(\n",
    "    workspace_id=launcher.workspace_id,\n",
    "    repository_directory=repository_directory,\n",
    "    notebookutils=notebookutils,\n",
    "    environment=ENVIRONMENT,\n",
    ")\n",
    "deployer.deploy_data_stores()\n",
    "\n",
    "# Step 3: Deploy compute and analytics\n",
    "print(\"\\n‚öôÔ∏è Deploying compute and analytics...\")\n",
    "deployer.deploy_compute_and_analytics()\n",
    "\n",
    "print(\"\\n‚úÖ Step-by-step deployment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f860aba",
   "metadata": {},
   "source": [
    "### Alternative 2: Selective Item Type Deployment\n",
    "\n",
    "Deploy only specific types of Fabric items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy only specific item types\n",
    "launcher.deploy_artifacts(\n",
    "    repository_directory=\".lakehouse/default/Files/src/workspace\", item_types=[\"Lakehouse\", \"Notebook\", \"Eventstream\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Selective deployment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee16a3",
   "metadata": {},
   "source": [
    "### Alternative 3: Using Individual Components\n",
    "\n",
    "For maximum control, use individual components directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric_launcher import FabricDeployer, GitHubDownloader, LakehouseFileManager, NotebookExecutor\n",
    "\n",
    "# 1. GitHub operations\n",
    "downloader = GitHubDownloader(REPO_OWNER, REPO_NAME, branch=BRANCH)\n",
    "downloader.download_and_extract_folder(extract_to=\"./src\", folder_to_extract=FOLDER_TO_EXTRACT)\n",
    "\n",
    "# 2. Fabric deployment\n",
    "deployer = FabricDeployer(\n",
    "    workspace_id=launcher.workspace_id,\n",
    "    repository_directory=\"./src/workspace\",\n",
    "    notebookutils=notebookutils,\n",
    "    environment=ENVIRONMENT,\n",
    ")\n",
    "deployer.deploy_all_in_stages()\n",
    "\n",
    "# 3. File operations\n",
    "file_mgr = LakehouseFileManager(notebookutils)\n",
    "file_mgr.upload_file_to_lakehouse(lakehouse_name=LAKEHOUSE_NAME, file_path=\"./config.json\", target_folder=\"config\")\n",
    "\n",
    "# 4. Notebook execution\n",
    "executor = NotebookExecutor(notebookutils)\n",
    "result = executor.run_notebook_synchronous(notebook_path=POST_DEPLOYMENT_NOTEBOOK, parameters=NOTEBOOK_PARAMETERS)\n",
    "\n",
    "print(\"‚úÖ Component-based deployment completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
